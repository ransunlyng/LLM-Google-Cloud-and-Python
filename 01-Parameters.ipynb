{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe54735-1c2f-4ab6-a8e0-19e743a135a3",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.pieriantraining.com/\" target=\"_blank\"><img src=\"../PTCenteredPurple.png\" alt=\"Pierian Training Logo\" /></a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeca1d-5190-4346-8599-9222dac6b4c1",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e518d5e-3493-4d22-8b29-92840b51f44c",
   "metadata": {},
   "source": [
    "### Customizing the Behavior of Text Models in Google Vertex AI's PaLM API\n",
    "\n",
    "When you're working with the text-bison model (and other models) through the PaLM API in Google Vertex AI, you have the ability to tweak its behavior to better suit your specific needs. This is done by adjusting various parameters that control how the model generates text. Let's break down what each of these parameters means and how you can use them:\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "1. **Temperature**: \n",
    "    - **What it Does**: Controls the \"creativity\" of the model's responses.\n",
    "    - **How to Use It**: A higher value (closer to 1) will make the model's output more random and creative. A lower value (closer to 0) will make the output more focused and deterministic.\n",
    "    - **Example**: If you set the temperature to 0.2, the model is more likely to generate safe and predictable text. On the other hand, a temperature of 0.8 would result in more diverse and unexpected outputs.\n",
    "\n",
    "2. **Max Output Tokens**: \n",
    "    - **What it Does**: Limits the length of the generated text.\n",
    "    - **How to Use It**: Set this parameter to the maximum number of tokens you want in the output. A token can be as short as one character or as long as one word.\n",
    "    - **Example**: If you set `max_output_tokens` to 50, the model will generate text that is no longer than 50 tokens.\n",
    "\n",
    "3. **Top_p**: \n",
    "    - **What it Does**: Controls the diversity of the next token based on cumulative probability.\n",
    "    - **How to Use It**: A higher value (closer to 1) means the model will consider a broader range of possible next tokens. A lower value (closer to 0) narrows down the options.\n",
    "    - **Example**: If you set `top_p` to 0.9, the model will consider a wider array of tokens for the next position in the sequence, making the output more varied.\n",
    "\n",
    "4. **Top_k**: \n",
    "    - **What it Does**: Limits the number of top candidates for the next token.\n",
    "    - **How to Use It**: A higher value will allow the model to sample from more possible next tokens, making the output more diverse. A lower value will restrict the model to a smaller set of highly probable next tokens.\n",
    "    - **Example**: If you set `top_k` to 40, the model will choose the next token from the top 40 most likely candidates.\n",
    "\n",
    "By understanding and adjusting these parameters, you can fine-tune the behavior of the text-bison@001 model to generate text that aligns with your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb3be7-3a63-4372-8713-78649a28e878",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Setting Up Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8513133f-0885-4e4e-a246-b108e4212ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6964679a-8744-49f0-9163-b8c7af910e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose this string based on Console Lecture\n",
    "model_name = 'text-bison'\n",
    "model = TextGenerationModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48e5c4-0f46-4f15-b458-3979c9c82eb8",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "## Max Output Tokens\n",
    "### Understanding the max_output_tokens Parameter in Google Vertex AI's PaLM API\n",
    "\n",
    "#### What Are Tokens?\n",
    "\n",
    "In the context of language models like text-bison@001, a \"token\" is a unit of text that the model reads. A token can be as small as a single character or as long as a word. It's crucial to understand the concept of tokens because both the input and output of the model are constrained by token limits.\n",
    "\n",
    "- **Token Size**: On average, a token is approximately four characters long.\n",
    "- **Token-to-Word Conversion**: About 100 tokens usually equate to roughly 60-80 words.\n",
    "\n",
    "#### What is the max_output_tokens Parameter?\n",
    "\n",
    "The `max_output_tokens` parameter sets the upper limit on the number of tokens that can be generated in the model's response. This parameter is especially useful when you want to control the length of the generated text.\n",
    "\n",
    "#### How Does max_output_tokens Affect the Generated Response?\n",
    "\n",
    "1. **Shorter Responses**: \n",
    "    - **Lower Values**: If you set `max_output_tokens` to a lower number (e.g., 10 or 20), the model will generate shorter responses.\n",
    "    - **Use-Cases**: This is useful for tasks like generating headlines, titles, or brief summaries.\n",
    "\n",
    "2. **Longer Responses**: \n",
    "    - **Higher Values**: Setting `max_output_tokens` to a higher number (e.g., 500 or 1024) allows the model to generate longer, more detailed responses.\n",
    "    - **Use-Cases**: This is beneficial for tasks like article writing, storytelling, or generating detailed explanations.\n",
    "\n",
    "3. **Default Setting**: \n",
    "    - The default value for `max_output_tokens` is 128, which usually results in a moderately sized output suitable for a variety of tasks.\n",
    "\n",
    "#### Additional Resources:\n",
    "\n",
    "For a more in-depth understanding of the `max_output_tokens` parameter and other settings, you can refer to the official Google Cloud Vertex AI documentation: [Model Parameters in Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models#text_model_parameters).\n",
    "\n",
    "By understanding how the `max_output_tokens` parameter works, you can better control the length and detail of the text generated by the model, making it more suited to your specific needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03ad7dd-4b9f-427c-bfb1-aebe899ca700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There once was a dog\n"
     ]
    }
   ],
   "source": [
    "max_output_tokens_val = 5\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=\"Write me a funny poem about dogs\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17860d2a-b3d0-4d1d-bc91-a9b1c5a226dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There once was a dog named Sparky,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_output_tokens_val = 10\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=\"Write me a funny poem about dogs\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6dc815c-7fe5-4cfa-bdb0-d1d3b48755fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There once was a dog named Sparky,\n",
      "Who was always quite playful and barky.\n",
      "He loved to chase balls,\n",
      "And run down the halls,\n",
      "And he never failed to make me happy.\n",
      "\n",
      "One day, Sparky was playing in the park,\n",
      "When he saw a squirrel and started to bark.\n",
      "He chased the squirrel up a tree,\n",
      "But the squirrel was too quick, you see,\n",
      "And Sparky came tumbling down with a spark.\n",
      "\n",
      "He landed in a big pile of mud,\n",
      "And he looked like a giant chocolate bud.\n",
      "I couldn't help but laugh,\n",
      "At my silly dog, Sparky,\n",
      "And his muddy, chocolate-y face.\n",
      "\n",
      "I helped Sparky clean up,\n",
      "And we went home for a big pup-cup.\n",
      "We cuddled up on the couch,\n",
      "And I read Sparky a story,\n",
      "And we both had a very happy day.\n",
      "\n",
      "So if you're ever feeling down,\n",
      "Just remember the story of Sparky the clown.\n",
      "He's sure to put a smile on your face,\n",
      "And make you feel happy in your space.\n"
     ]
    }
   ],
   "source": [
    "max_output_tokens_val = 2000\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=\"Write me a funny poem about dogs\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f7e8de-b3dd-41eb-a8f6-92bf9a58d39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There once was a dog named Sparky,\n",
      "Who was always quite playful and barky.\n",
      "He loved to chase balls,\n",
      "And run down the halls,\n",
      "And he never failed to make me happy.\n",
      "\n",
      "One day, Sparky was playing in the park,\n",
      "When he saw a squirrel and started to bark.\n",
      "He chased the squirrel up a tree,\n",
      "But the squirrel was too quick, you see,\n",
      "And Sparky came tumbling down with a spark.\n",
      "\n",
      "He landed in a big pile of mud,\n",
      "And he looked like a giant chocolate bud.\n",
      "I couldn't help but laugh,\n",
      "At my silly dog, Sparky,\n",
      "And his muddy, chocolate-y face.\n",
      "\n",
      "I helped Sparky clean up,\n",
      "And we went home for a big pup-cup.\n",
      "We cuddled up on the couch,\n",
      "And I read Sparky a story,\n",
      "And we both had a very happy day.\n",
      "\n",
      "So if you're ever feeling down,\n",
      "Just remember the story of Sparky the clown.\n",
      "He's sure to put a smile on your face,\n",
      "And make you feel happy in your space.\n"
     ]
    }
   ],
   "source": [
    "max_output_tokens_val = 1000\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=\"Write me a funny poem about dogs\",\n",
    "    max_output_tokens=max_output_tokens_val,\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6ac4c-26b2-45cc-8eea-576f1cd60bdf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb10efd-865d-4bad-b8b5-d1a2ffa83b85",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04deaa2-e3c6-4d9d-b166-1bd9de5d5dc6",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "### Understanding the Temperature Parameter in Google Vertex AI's PaLM API\n",
    "\n",
    "#### What is the Temperature Parameter?\n",
    "\n",
    "The temperature parameter is a crucial setting in the text-bison@001 model available through Google Vertex AI's PaLM API. It plays a role in the sampling process during text generation, particularly when the `top_p` and `top_k` parameters are also in play. Essentially, the temperature parameter controls how \"random\" or \"creative\" the model's generated text will be.\n",
    "\n",
    "#### How Does Temperature Affect the Generated Response?\n",
    "\n",
    "1. **Low Temperature Values (e.g., 0.0 - 0.2)**:\n",
    "    - **Deterministic Output**: If you set the temperature to 0, the model will always choose the token with the highest probability, resulting in a deterministic output.\n",
    "    - **Less Open-Ended**: Lower temperatures are ideal for scenarios where you need a more straightforward, predictable response.\n",
    "    - **Common Phrases**: With a low temperature, the model is more likely to generate commonly used words or phrases.\n",
    "\n",
    "2. **High Temperature Values (e.g., 0.8 - 1.0)**:\n",
    "    - **Creative Output**: Higher temperatures make the model more \"adventurous\" in its choice of tokens, leading to more diverse and creative text.\n",
    "    - **Exploratory**: You're more likely to see rare or unusual words and phrases in the output.\n",
    "  \n",
    "3. **Moderate Temperature Values (e.g., 0.2 - 0.7)**:\n",
    "    - **Balanced Output**: These settings offer a balance between creativity and determinism, making them a good starting point for many use cases.\n",
    "\n",
    "#### Important Considerations:\n",
    "\n",
    "- **Risk of Nonsensical Output**: While a high temperature can make the text more interesting, it also increases the risk of generating text that doesn't make sense or is inappropriate. This phenomenon is sometimes referred to as \"hallucinations.\"\n",
    "  \n",
    "- **Starting Point**: For most applications, a good starting point is a temperature of 0.2. This offers a balance between generating text that is both coherent and slightly creative.\n",
    "\n",
    "By understanding how the temperature parameter works and its impact on text generation, you can better tailor the model's output to meet your specific needs. Always remember to use this parameter thoughtfully, considering the context and purpose of your text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db1bb343-3dd7-4601-ba5c-bcbc0945d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I was walking and then a most peculiar thing happened. I saw a man walking towards me, but as he got closer, I realized that he was not a man at all. He was a giant spider, with eight long legs and a hairy body. I was so shocked that I couldn't move. I just stood there, frozen in fear, as the spider got closer and closer.\n",
      "\n",
      "Finally, the spider stopped in front of me. It raised its front legs and stared at me with its eight red eyes. I couldn't believe what I was seeing. I was about to be eaten by a giant spider!\n",
      "\n",
      "But then, the spider did something unexpected. It turned and walked away. I was so relieved that I almost cried. I didn't know what had just happened, but I wasn't going to question it. I just turned and ran away as fast as I could.\n",
      "\n",
      "I didn't stop running until I was safe at home. I told my family what had happened, and they were just as shocked as I was. We all agreed that it was a very strange experience, and we're still not sure what to make of it.\n",
      "\n",
      "But one thing is for sure: I'm never going to forget it.\n"
     ]
    }
   ],
   "source": [
    "## RE-RUN THIS CELL MULTIPLE TIMES< NOTICE THE OUTPUT!\n",
    "temp_val = 0.0\n",
    "prompt  = \"Continue this story: 'I was walking and then a most peculiar thing happened.'\"\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=prompt ,\n",
    "    temperature=temp_val,\n",
    "     max_output_tokens=1000\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc45ff70-7224-48fe-bba8-8e0b9db0b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I was walking home from school one day when I saw a most peculiar thing. A large, white rabbit wearing a waistcoat and a pocket watch was running very fast. The rabbit looked like it was in a hurry, so I decided to follow it. \n",
      "The rabbit led me down a hole, and I fell and fell and fell. When I landed, I found myself in a strange and wonderful world. There were talking animals, magical creatures, and all sorts of other strange and wonderful things. I had many adventures in that world, and I learned a lot about myself along the way. \n",
      "One day, I decided it was time to go home. I said goodbye to my new friends, and I stepped back through the hole. When I landed, I was back in my own backyard. I looked up and saw the white rabbit running away. I smiled, knowing that I would never forget my adventures in Wonderland.\n"
     ]
    }
   ],
   "source": [
    "## RE-RUN THIS CELL MULTIPLE TIMES< NOTICE THE OUTPUT!\n",
    "temp_val = 1.0\n",
    "prompt  = \"Continue this story: 'I was walking and then a most peculiar thing happened.'\"\n",
    "\n",
    "\n",
    "response = model.predict(\n",
    "    prompt=prompt ,\n",
    "    temperature=temp_val,\n",
    "     max_output_tokens=1000\n",
    ")\n",
    " \n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccdadb-31f2-4364-aeab-dbea8edde170",
   "metadata": {},
   "source": [
    "## Top-K\n",
    "\n",
    "### Understanding the top_k Parameter in Google Vertex AI's PaLM API\n",
    "\n",
    "#### What is the top_k Parameter?\n",
    "\n",
    "The `top_k` parameter controls the number of most probable tokens that the model considers when generating the next token in a sequence. This is known as \"top-k sampling.\"\n",
    "\n",
    "- **Greedy Decoding**: If `top_k` is set to 1, the model will always choose the most probable token, a method also known as greedy decoding.\n",
    "- **Top-k Sampling**: If `top_k` is set to a number greater than 1 (e.g., 3, 10, or 40), the model will consider that many of the most probable tokens for the next position in the generated text.\n",
    "\n",
    "#### How Does It Work in Conjunction with Other Parameters?\n",
    "\n",
    "The `top_k` parameter often works in tandem with other parameters like `top_p` and `temperature`:\n",
    "\n",
    "1. **Step 1**: The model first identifies the `top_k` tokens with the highest probabilities.\n",
    "2. **Step 2**: These tokens are then filtered based on the `top_p` value, which sets a cumulative probability cutoff.\n",
    "3. **Step 3**: Finally, one of these filtered tokens is selected based on the `temperature` setting, which controls the randomness of the choice.\n",
    "\n",
    "#### How Does top_k Affect the Generated Response?\n",
    "\n",
    "1. **Lower top_k Values (e.g., 1 - 10)**:\n",
    "    - **Less Random**: The model will generate more predictable and focused text because it's limited to a smaller set of highly probable tokens.\n",
    "    - **Use-Cases**: This is useful for tasks that require specific and accurate responses, such as FAQ generation or summarization.\n",
    "\n",
    "2. **Higher top_k Values (e.g., 20 - 40)**:\n",
    "    - **More Random**: The model has a broader range of tokens to choose from, making the output more diverse and creative.\n",
    "    - **Use-Cases**: This is beneficial for creative writing, brainstorming, or any task where a diverse range of ideas is desired.\n",
    "\n",
    "3. **Default Setting**:\n",
    "    - The default value for `top_k` is 40, which provides a good balance between predictability and creativity for most general-purpose tasks.\n",
    "\n",
    "By understanding how the `top_k` parameter works, you can better control the randomness and focus of the text generated by the model, tailoring it to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4745942-f7ad-4195-968e-55ab88b00834",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Plan a trip to Rome\"\n",
    "model.predict(prompt=prompt,max_output_tokens=1024,top_k=1,temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08422ea3-2295-463d-a717-0008503a2982",
   "metadata": {},
   "source": [
    "## Top-P\n",
    "### Understanding the top_p Parameter in Google Vertex AI's PaLM API\n",
    "\n",
    "#### What is the top_p Parameter?\n",
    "\n",
    "The `top_p` parameter, also known as \"nucleus sampling,\" is a setting that controls the diversity of the tokens selected by the model during text generation. It adjusts the probability distribution of the next token based on a cumulative probability cutoff.\n",
    "\n",
    "- **Cumulative Probability**: This is the sum of probabilities of individual tokens, sorted in descending order.\n",
    "- **Cutoff Probability**: The `top_p` value acts as a cutoff, allowing only tokens whose cumulative probability exceeds this value to be considered for the next position in the generated text.\n",
    "\n",
    "#### How Does It Work? An Example:\n",
    "\n",
    "Let's say we have three tokens: A, B, and C, with probabilities of 0.3, 0.2, and 0.1, respectively. If `top_p` is set to 0.5, then:\n",
    "\n",
    "- The cumulative probability for A and B is 0.3 + 0.2 = 0.5, which matches the `top_p` value.\n",
    "- Token C, with a probability of 0.1, is excluded because adding it would exceed the `top_p` value.\n",
    "- The model will then choose between A and B for the next token, based on other parameters like `temperature`.\n",
    "\n",
    "#### How Does top_p Affect the Generated Response?\n",
    "\n",
    "1. **Higher top_p Values (e.g., 0.8 - 1.0)**:\n",
    "    - **Diverse Outputs**: The model can sample from a larger set of tokens, making the output more varied and interesting.\n",
    "    - **Use-Cases**: This is useful when you want creative or exploratory text, like brainstorming sessions or storytelling.\n",
    "\n",
    "2. **Lower top_p Values (e.g., 0.2 - 0.5)**:\n",
    "    - **Predictable Outputs**: The model is constrained to a smaller set of highly probable tokens, resulting in more focused and predictable text.\n",
    "    - **Use-Cases**: This is beneficial for tasks that require precise and accurate information, like summarizing a technical document.\n",
    "\n",
    "3. **Default Setting**:\n",
    "    - The default value for `top_p` is 0.95, which offers a good balance between diversity and predictability for most general-purpose tasks.\n",
    "\n",
    "By understanding the `top_p` parameter, you can fine-tune the diversity and predictability of the text generated by the model, making it more aligned with your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4834759-0ce2-4a2c-84a5-8f3c9381b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.0]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a crazy story: \" \n",
    "model.predict(prompt=prompt,max_output_tokens=1024,top_k=40,top_p=1.0,temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0014e-4582-47cf-8c86-62d994c2f0b2",
   "metadata": {},
   "source": [
    "## Stop Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aceeff-39b2-459f-a640-86391eaa357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(prompt=\"Give me a numbered list of vegetables\",\n",
    "             stop_sequences=['4.'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
